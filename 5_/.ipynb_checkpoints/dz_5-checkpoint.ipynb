{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14682339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import re\n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d05c65a",
   "metadata": {},
   "source": [
    "# Задание 1. Обучите нейронную сеть решать шифр цезаря."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bdd96ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция кодирования\n",
    "# c lowcase проще, но с большими буквами лучше демонстрируется как тяжелее предсказываются редкие символы, \n",
    "# и, особенно, последовательности редких символов\n",
    "def make_cesar(string, n):\n",
    "    alf = list('АБВГДЕЁЖЗИЙКЛМНОПРСТУФЧХЦШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюя., ')\n",
    "    size = len(alf)\n",
    "    char_num =  {key: item for item, key in enumerate(alf)}  \n",
    "    char_num['none'] = len(char_num) # +1\n",
    "    num_char =  {item: key for item, key in enumerate(alf)}\n",
    "    num_char[len(num_char)] = 'none'\n",
    "    result = []\n",
    "    for i in string:\n",
    "        result.append(num_char[(char_num[i] + n)%size])\n",
    "    return ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1c6ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Chehov.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c4e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(\"[-,—!\\n«»!?:;()]\", ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c1bb11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ЕтчутДЪйъужДНУТаЪДДПузиеДжДзшёйхтцпусДзухуийДХВДфхнймлнйДлеружернц,ДтеДцпшпшДнДуитууёхемнйДлнмтнДДчу'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cesar = make_cesar(text, 5)\n",
    "cesar[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a95e2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# готовим нашу модель\n",
    "# создаем char_num и num_char\n",
    "alf = list('АБВГДЕЁЖЗИЙКЛМНОПРСТУФЧХЦШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюя., ')\n",
    "char_num =  {key: item for item, key in enumerate(alf)}  \n",
    "char_num['none'] = len(char_num)\n",
    "num_char =  {item: key for item, key in enumerate(alf)}\n",
    "num_char[len(num_char)] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "555a38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50 # длина строки в тензоре\n",
    "X = torch.zeros((len(cesar) // MAX_LEN + 1, MAX_LEN), dtype=int) # создаем тензоры куда будем класть оцифрованные символы\n",
    "Y = torch.zeros((len(cesar) // MAX_LEN + 1, MAX_LEN), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdc56324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполняем созданные тензоры значениями (кодированными буквами)\n",
    "for i in range(len(cesar) // MAX_LEN + 1):\n",
    "    for j in range(MAX_LEN):\n",
    "        if (i * MAX_LEN + j) >= len(cesar):\n",
    "            X[i, j] = char_num['none']\n",
    "            continue\n",
    "        if (i * MAX_LEN + j) >= len(text):\n",
    "            Y[i, j] = char_num['none']\n",
    "            continue\n",
    "        X[i, j] = char_num[cesar[i * MAX_LEN + j]]\n",
    "        Y[i, j] = char_num[text[i * MAX_LEN + j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "220fa195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем класс модели\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.embbeding = torch.nn.Embedding(len(num_char), 28)\n",
    "        self.rnn = torch.nn.RNN(28, 128, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(128, len(char_num))       \n",
    "\n",
    "        \n",
    "    def forward(self, sentences, state=None):\n",
    "        embed = self.embbeding(sentences)\n",
    "        o, s = self.rnn(embed)\n",
    "        out = self.linear(o)\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "018ec855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем модель\n",
    "model = Network()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05e551a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d54b129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 22.829, Train loss: 4.226\n",
      "Epoch 100. Time: 0.312, Train loss: 1.095\n",
      "Epoch 200. Time: 0.299, Train loss: 0.221\n",
      "Epoch 300. Time: 0.302, Train loss: 0.083\n",
      "Epoch 400. Time: 0.301, Train loss: 0.043\n",
      "Epoch 500. Time: 0.308, Train loss: 0.026\n",
      "Epoch 600. Time: 0.307, Train loss: 0.018\n",
      "Epoch 700. Time: 0.302, Train loss: 0.013\n",
      "Epoch 800. Time: 0.320, Train loss: 0.010\n",
      "Epoch 900. Time: 0.307, Train loss: 0.008\n",
      "Epoch 1000. Time: 0.303, Train loss: 0.006\n",
      "Epoch 1100. Time: 0.314, Train loss: 0.005\n",
      "Epoch 1200. Time: 0.317, Train loss: 0.004\n",
      "Epoch 1300. Time: 0.306, Train loss: 0.003\n",
      "Epoch 1400. Time: 0.300, Train loss: 0.003\n",
      "Epoch 1500. Time: 0.301, Train loss: 0.002\n",
      "Epoch 1600. Time: 0.304, Train loss: 0.002\n",
      "Epoch 1700. Time: 0.305, Train loss: 0.002\n",
      "Epoch 1800. Time: 0.305, Train loss: 0.002\n",
      "Epoch 1900. Time: 0.302, Train loss: 0.001\n"
     ]
    }
   ],
   "source": [
    "# обучаем модель\n",
    "X = X.cuda()\n",
    "Y = Y.cuda()\n",
    "\n",
    "model.train()    \n",
    "for ep in range(2000):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0        \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    answers = model(X)\n",
    "    loss = criterion(answers.permute((0,2,1)), Y)\n",
    "    train_loss += loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_passed += 1\n",
    "    if ep%100 == 0:\n",
    "        print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, (time.time() - start) * 100, train_loss / train_passed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a87d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверка модели на другом тесте\n",
    "with open('saltan.txt', 'r') as f:\n",
    "    text1 = f.read()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4da2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = re.sub(\"[-,—!\\n«»!?:;()\\\"\\']\", ' ', text1)\n",
    "cesar_text1 = make_cesar(text1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ee9b794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ЕрйпцетихДХйхзййжньДФшэпнтВДХпемпеДуДыехйДХерчетйДДДХПЕМПЕДУДЬЕЧЙДХЕРЦЕТЙДДУДХаТЙДЙЗУДХРЕЖТУСДНДСУЗШ'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cesar_text1[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64522fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tensor(text, char_num, MAX_LEN):\n",
    "    X = torch.zeros((len(text) // MAX_LEN + 1, MAX_LEN), dtype=int)\n",
    "    for i in range(len(text) // MAX_LEN + 1):\n",
    "        for j in range(MAX_LEN):\n",
    "            if (i * MAX_LEN + j) >= len(text):\n",
    "                X[i, j] = char_num['none']\n",
    "                continue\n",
    "            X[i, j] = char_num[text[i * MAX_LEN + j]]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8a0477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(text, model, char_num, MAX_LEN):\n",
    "    X = text_tensor(text, char_num, MAX_LEN)\n",
    "    model.eval()\n",
    "    model = model.cuda()\n",
    "    X = X.cuda()\n",
    "    out = model(X)\n",
    "    res = []\n",
    "    for item in out:   \n",
    "        for i in item:\n",
    "            char = num_char[torch.argmax(i).item()]\n",
    "            res.append(num_char[torch.argmax(i).item()])\n",
    "    return ''.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae77bdd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Александр Сергеевич Пушкин. Сказка о царе Салтане   СКАЗКв О тАцЕ САЛТАНЕ  О СЫНЕ ЕГО СЛАВНОМ И МОГУЧЕМ бОГАТЫмЕ КНЯЗЕ ГВИДОНЕ САЛТАНОВИЧЕ И О ПуЕКуАСНОТ тАцЕВНЕ ЛЕбЕДИ   Три девицы под окном Пряли поздно вечерком.  Кабы я была царица   Говорит одна девица   То на весь крещеный мир Приготовила б я пир .    Кабы я была царица   Говорит ее сестрица   То на весь бы мир одна Наткала я полотна .    Кабы я была царица   Третья молвила сестрица   Я б для батюшки царя уодила богатыря .  Только вымолвить успела  Дверь тихонько заскрыпела  И в светлицу входит царь  Стороны той государь. Во все время разговора Он стоял позадь забора  уечь последней по всему Полюбилася ему.  Здравствуй  красная девица   Говорит он   будь царица И роди богатыря Мне к исходу сентября. Вы ж  голубушки сестрицы  Выбирайтесь из светлицы. Поезжайте вслед за мной  Вслед за мной и за сестрой  будь одна из вас ткачиха  А другая повариха .  В сени вышел царь отец. Все пустились во дворец. Гарь недолго собирался  В тот же вечер обвенчался. Гарь Салтан за пир честной Сел с царицей молодой  А потом честные гости На кровать слоновой кости Положили молодых И оставили одних. В кухне злится повариха  Плачет у станка ткачиха   И завидуют оне Государевой жене. А царица молодая  Дела вдаль не отлагая  С первой ночи понесла.  В те поры война была. Гарь Салтан  с женой простяся  На добра коня садяся  Ей наказывал себя Поберечь  его любя.  Между тем  как он далеко бьется долго и жестоко  Наступает срок родин  Сына бог им дал в аршин  И царица над ребенком  Как орлица над орленком  Шлет с письмом она гонца  Чтоб обрадовать отца. А ткачиха с поварихой  С сватьей бабой бабарихой Извести ее хотят  Перенять гонца велят  Сами шлют гонца другого Вот с чем от слова до слова   уодила царица в ночь Не то сына  не то дочь  Не мышонка  не лягушку  А неведому зверюшку .  Как услышал царь отец  Что донес ему гонец  В гневе начал он чудесить И гонца хотел повесить  Но  смягчившись на сей раз  Дал гонцу такой приказ   Ждать царева в'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(cesar_text1, model, char_num, MAX_LEN)[:2000]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e864345a",
   "metadata": {},
   "source": [
    "Вывод: текст декодируется. Текст декодируется хорошо, понятно. Редкие символы декодируются хуже, последовательности редких символов декодируются еще хуже. Улучшить ситуацию может увеличение количества эпох обучения, а также использование при создании nn.Embedding параметр scale_grad_by_freq=True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe8018",
   "metadata": {},
   "source": [
    "# Задание 2."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b515e8cf",
   "metadata": {},
   "source": [
    "Выполнить практическую работу из лекционного ноутбука.\n",
    "а) построить RNN-ячейку на основе полносвязных слоев\n",
    "б) применить построенную ячейку для генерации текста с выражениями героев сериала “Симпсоны"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071c761f",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38bab71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>timestamp_in_ms</th>\n",
       "      <th>speaking_line</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10368</td>\n",
       "      <td>35</td>\n",
       "      <td>29</td>\n",
       "      <td>Lisa Simpson: Maggie, look. What's that?</td>\n",
       "      <td>235000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Simpson Home</td>\n",
       "      <td>Maggie, look. What's that?</td>\n",
       "      <td>maggie look whats that</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10369</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "      <td>Lisa Simpson: Lee-mur. Lee-mur.</td>\n",
       "      <td>237000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Simpson Home</td>\n",
       "      <td>Lee-mur. Lee-mur.</td>\n",
       "      <td>lee-mur lee-mur</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10370</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>Lisa Simpson: Zee-boo. Zee-boo.</td>\n",
       "      <td>239000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Simpson Home</td>\n",
       "      <td>Zee-boo. Zee-boo.</td>\n",
       "      <td>zee-boo zee-boo</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10372</td>\n",
       "      <td>35</td>\n",
       "      <td>33</td>\n",
       "      <td>Lisa Simpson: I'm trying to teach Maggie that ...</td>\n",
       "      <td>245000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Simpson Home</td>\n",
       "      <td>I'm trying to teach Maggie that nature doesn't...</td>\n",
       "      <td>im trying to teach maggie that nature doesnt e...</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10374</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>Lisa Simpson: It's like an ox, only it has a h...</td>\n",
       "      <td>254000</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Simpson Home</td>\n",
       "      <td>It's like an ox, only it has a hump and a dewl...</td>\n",
       "      <td>its like an ox only it has a hump and a dewlap...</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id  episode_id  number  \\\n",
       "0           0  10368          35      29   \n",
       "1           1  10369          35      30   \n",
       "2           2  10370          35      31   \n",
       "3           3  10372          35      33   \n",
       "4           4  10374          35      35   \n",
       "\n",
       "                                            raw_text  timestamp_in_ms  \\\n",
       "0           Lisa Simpson: Maggie, look. What's that?           235000   \n",
       "1                    Lisa Simpson: Lee-mur. Lee-mur.           237000   \n",
       "2                    Lisa Simpson: Zee-boo. Zee-boo.           239000   \n",
       "3  Lisa Simpson: I'm trying to teach Maggie that ...           245000   \n",
       "4  Lisa Simpson: It's like an ox, only it has a h...           254000   \n",
       "\n",
       "   speaking_line  character_id  location_id raw_character_text  \\\n",
       "0           True             9          5.0       Lisa Simpson   \n",
       "1           True             9          5.0       Lisa Simpson   \n",
       "2           True             9          5.0       Lisa Simpson   \n",
       "3           True             9          5.0       Lisa Simpson   \n",
       "4           True             9          5.0       Lisa Simpson   \n",
       "\n",
       "  raw_location_text                                       spoken_words  \\\n",
       "0      Simpson Home                         Maggie, look. What's that?   \n",
       "1      Simpson Home                                  Lee-mur. Lee-mur.   \n",
       "2      Simpson Home                                  Zee-boo. Zee-boo.   \n",
       "3      Simpson Home  I'm trying to teach Maggie that nature doesn't...   \n",
       "4      Simpson Home  It's like an ox, only it has a hump and a dewl...   \n",
       "\n",
       "                                     normalized_text  word_count  \n",
       "0                             maggie look whats that         4.0  \n",
       "1                                    lee-mur lee-mur         2.0  \n",
       "2                                    zee-boo zee-boo         2.0  \n",
       "3  im trying to teach maggie that nature doesnt e...        24.0  \n",
       "4  its like an ox only it has a hump and a dewlap...        18.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4fc8bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['maggie look whats that',\n",
       " 'lee-mur lee-mur',\n",
       " 'zee-boo zee-boo',\n",
       " 'im trying to teach maggie that nature doesnt end with the barnyard i want her to have all the advantages that i didnt have',\n",
       " 'its like an ox only it has a hump and a dewlap hump and dew-lap hump and dew-lap',\n",
       " 'you know his blood type how romantic',\n",
       " 'oh yeah whats my shoe size',\n",
       " 'ring',\n",
       " 'yes dad',\n",
       " 'ooh look maggie what is that do-dec-ah-edron dodecahedron']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = df['normalized_text'].tolist()\n",
    "phrases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34556f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [[c for c in ph] for ph in phrases if type(ph) is str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a65a1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS = set('abcdefghijklmnopqrstuvwxyz ')\n",
    "INDEX_TO_CHAR = ['none'] + [w for w in CHARS]\n",
    "CHAR_TO_INDEX = {w: i for i, w in enumerate(INDEX_TO_CHAR)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94158fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50\n",
    "X = torch.zeros((len(text), MAX_LEN), dtype=int)\n",
    "for i in range(len(text)):\n",
    "    for j, w in enumerate(text[i]):\n",
    "        if j >= MAX_LEN:\n",
    "            break\n",
    "        X[i, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5afc48cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# модель из лекционного ноутбука, для сравнения\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.embbeding = torch.nn.Embedding(len(INDEX_TO_CHAR), 28)\n",
    "        self.rnn = torch.nn.RNN(28, 128, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(128, len(CHAR_TO_INDEX))\n",
    "        \n",
    "    def forward(self, sentences, state=None):\n",
    "        embed = self.embbeding(sentences)\n",
    "        o, s = self.rnn(embed)\n",
    "        out = self.linear(o)\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03577152",
   "metadata": {},
   "source": [
    "### Вариант 1, неправильный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "031bb47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вариант, когда сохраняется  веса каждой ячейки скрытого слоя - \n",
    "# т.е для каждого порядкового номера x - свой nn.Linear() . \n",
    "# Данный вариант показывает большую ошибку и обучается плохо.\n",
    "\n",
    "class Net_3(nn.Module):\n",
    "    def __init__(self, E, H, out_size):           \n",
    "        super(Net_3, self).__init__()      \n",
    "        self.H = H\n",
    "        self.E = E\n",
    "        self.lin = [] # nn.Linear(self.E + H, H)\n",
    "        self.emb = nn.Embedding(out_size, E)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.linear = torch.nn.Linear(self.H, out_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # B- размер батча, L- длина входного вектора, H- размер скрытого слоя,\n",
    "        (B, L), H =  x.size(), self.H \n",
    "        \n",
    "        # инициализируем начальный скрытый слой нулями\n",
    "        h0 = torch.zeros(B,H, device = x.device)  \n",
    "        \n",
    "#         мы не знаем заранее длину последовательности, создаем \"список ячеек\" по длине входных векторов\n",
    "#         чтоб в каждой ячейке были свои веса, (делаем извращение с линейными слоеями, проще с весами было написать.\n",
    "#         что-то типа Hn =torch.tanh(torch.addmm(B_ih, x,  W_ih.t()) + torch.addmm(B_hh, Hn, W_hh.t())) и,наверное, \n",
    "#         быстрее бы было.\n",
    "        self.lin = [nn.Sequential(nn.Linear(self.E + H, H), nn.Tanh()).to(device=x.device) for _ in range(L)]\n",
    "        x = self.emb(x)  \n",
    "        result = []\n",
    "\n",
    "        for ind, item in enumerate(x.permute(1, 0, 2)):    \n",
    "            xh = torch.cat([item, h0], dim=1) # конкатенируем x в эмбединге со скрытым слоем из предыдущего шага \n",
    "        # в первом шаге, с инициализирующим h0, получается  горизонтальная связь.\n",
    "\n",
    "            o = self.lin[ind](xh) # фактически реализация линейного слоя это перемножение мартицы x (у нас матрица = x + h(скрытый)) \n",
    "#             с матрицей весов (у нас 2 конкатенированные матрицы W + H) + bias. \n",
    "#              В конце, приделан гиперборический тангес,чтоб избежать роста значений скрытого слоя при умножении матриц \n",
    "\n",
    "            result.append(o) \n",
    "            h0 = o\n",
    "        out = torch.stack(result).permute(1, 0, 2)\n",
    "        out = self.linear(out)\n",
    "        return out # ,h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bca14cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net_3(28, 100, 28)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.1)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "accbaf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 2.012, Train loss: 2.928\n",
      "Epoch 1. Time: 1.910, Train loss: 2.655\n",
      "Epoch 2. Time: 1.934, Train loss: 2.607\n",
      "Epoch 3. Time: 1.881, Train loss: 2.585\n",
      "Epoch 4. Time: 1.919, Train loss: 2.572\n",
      "Epoch 5. Time: 1.898, Train loss: 2.563\n",
      "Epoch 6. Time: 1.900, Train loss: 2.557\n",
      "Epoch 7. Time: 1.931, Train loss: 2.553\n",
      "Epoch 8. Time: 1.934, Train loss: 2.549\n",
      "Epoch 9. Time: 1.873, Train loss: 2.546\n"
     ]
    }
   ],
   "source": [
    "for ep in range(10):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "    \n",
    "    X = X.cuda()\n",
    "\n",
    "    for i in range(int(len(X) / 100)):\n",
    "        batch = X[i * 100:(i + 1) * 100]\n",
    "        X_batch = batch[:, :-1]\n",
    "        Y_batch = batch[:, 1:].flatten()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        answers= model(X_batch)\n",
    "        answers = answers.view(-1, len(INDEX_TO_CHAR))\n",
    "\n",
    "        loss = criterion(answers, Y_batch)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "\n",
    "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef971474",
   "metadata": {},
   "source": [
    "### Вариант 2, правильный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09d33637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# В данном варианте - используется линейный слой общий для всех скрытых ячеек, и веса общие\n",
    "# данный вариант показывает лучший результат, по сравнению с первым вариантом\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, E, H, out_size):           \n",
    "        super(Net, self).__init__()      \n",
    "        self.H = H\n",
    "        self.E = E\n",
    "        self.lin = nn.Linear(self.E + H, H)\n",
    "        self.emb = nn.Embedding(out_size, E)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.linear = torch.nn.Linear(self.H, out_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        (B, L), H =  x.size(), self.H \n",
    "        \n",
    "        h0 = torch.zeros(B,H, device = x.device)\n",
    "            \n",
    "        h = [h0]\n",
    "        x = self.emb(x)  \n",
    "\n",
    "        for item in x.permute(1, 0, 2):    \n",
    "            xh = torch.cat([item, h[-1]], dim=1) # конкатенируем x в эмбединге со скрытым слоем из предsдущего шага \n",
    "        # (т.е горизонтальная связь)\n",
    "\n",
    "            o = self.lin(xh) # фактически реализация линейного слоя это перемножение мартицы x (у нас x + h(скрытый)) \n",
    "    #     с матрицей весов (у нас 2 конкатенированные матрицы W + H) + bias. Остается приделать гиперборический тангес.\n",
    "            o = self.tanh(o)    \n",
    "            h.append(o) \n",
    "            out = torch.stack(h[1:]).permute(1, 0, 2)\n",
    "            h0 = o\n",
    "\n",
    "        out = self.linear(out)\n",
    "        return(out)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96b37bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(28, 100, 28)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.1)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7909ad",
   "metadata": {},
   "source": [
    "### генерация текста для варианта № 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95291a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 1.345, Train loss: 1.981\n",
      "Epoch 1. Time: 1.314, Train loss: 1.719\n",
      "Epoch 2. Time: 1.277, Train loss: 1.648\n",
      "Epoch 3. Time: 1.275, Train loss: 1.606\n",
      "Epoch 4. Time: 1.272, Train loss: 1.576\n",
      "Epoch 5. Time: 1.273, Train loss: 1.552\n",
      "Epoch 6. Time: 1.276, Train loss: 1.533\n",
      "Epoch 7. Time: 1.311, Train loss: 1.515\n",
      "Epoch 8. Time: 1.322, Train loss: 1.500\n",
      "Epoch 9. Time: 1.380, Train loss: 1.486\n"
     ]
    }
   ],
   "source": [
    "for ep in range(10):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "    \n",
    "    X = X.cuda()\n",
    "\n",
    "    for i in range(int(len(X) / 100)):\n",
    "        batch = X[i * 100:(i + 1) * 100]\n",
    "        X_batch = batch[:, :-1]\n",
    "        Y_batch = batch[:, 1:].flatten()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        answers= model(X_batch)\n",
    "        answers = answers.view(-1, len(INDEX_TO_CHAR))\n",
    "        loss = criterion(answers, Y_batch)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "\n",
    "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83488d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model):\n",
    "    sentence = ['h', 'e', 'l', 'l', 'o']\n",
    "    \n",
    "    x = torch.zeros((1, len(sentence)), dtype=int)\n",
    "    x = x.cuda()\n",
    "    \n",
    "    for j, w in enumerate(sentence):\n",
    "        if j >= MAX_LEN:\n",
    "            break\n",
    "        x[0, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])\n",
    "        \n",
    "    for i in range(MAX_LEN):\n",
    "        o = model(x)\n",
    "        w = torch.argmax(o[-1, -1,: ], keepdim=True)\n",
    "        x = torch.cat([x, w.unsqueeze(0).unsqueeze(1)], axis=1)\n",
    "        ww = INDEX_TO_CHAR[w]\n",
    "\n",
    "        if ww == 'none':\n",
    "            break\n",
    "\n",
    "        sentence.append(ww)\n",
    "            \n",
    "    return ''.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea4486ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hellon the the the the the the the the the the the the '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "450e0c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 1.379, Train loss: 1.473\n",
      "hellon the the the the the the the the the the the the \n",
      "EPOCH 9, loss: 1.383Epoch 10. Time: 1.344, Train loss: 1.375\n",
      "hellong the so the so the so the so the so the so the s\n",
      "EPOCH 19, loss: 1.314Epoch 20. Time: 1.307, Train loss: 1.308\n",
      "hello the so the so the so the so the so the so the so \n",
      "EPOCH 29, loss: 1.266Epoch 30. Time: 1.318, Train loss: 1.262\n",
      "hello so the some i stor the some i stor the some i sto\n",
      "EPOCH 39, loss: 1.231Epoch 40. Time: 1.394, Train loss: 1.228\n",
      "hellot the some i can the some i can the some i can the\n",
      "EPOCH 49, loss: 1.205Epoch 50. Time: 1.394, Train loss: 1.202\n",
      "hello so the some of the some of the some of the some o\n",
      "EPOCH 59, loss: 1.184Epoch 60. Time: 1.337, Train loss: 1.182\n",
      "hello it so the some of the some of the some of the som\n",
      "EPOCH 69, loss: 1.167Epoch 70. Time: 1.368, Train loss: 1.165\n",
      "hello it so the something the something the something t\n",
      "EPOCH 79, loss: 1.152Epoch 80. Time: 1.410, Train loss: 1.151\n",
      "hello it so the something to the something to the somet\n",
      "EPOCH 89, loss: 1.140Epoch 90. Time: 1.400, Train loss: 1.138\n",
      "hello it so the something to the something to the somet\n",
      "EPOCH 99, loss: 1.129Epoch 100. Time: 1.316, Train loss: 1.128\n",
      "hello the something to the something to the something t\n"
     ]
    }
   ],
   "source": [
    "for ep in range(101):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "    \n",
    "    X = X.cuda()\n",
    "\n",
    "    for i in range(int(len(X) / 100)):\n",
    "        batch = X[i * 100:(i + 1) * 100]\n",
    "        X_batch = batch[:, :-1]\n",
    "        Y_batch = batch[:, 1:].flatten()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        answers = model(X_batch)\n",
    "        answers = answers.view(-1, len(INDEX_TO_CHAR))\n",
    "        loss = criterion(answers, Y_batch)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "    \n",
    "    if ep%10 == 0:\n",
    "        print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))\n",
    "        s = generate_sentence(model)\n",
    "        print(s)\n",
    "    else:\n",
    "        print(f'\\rEPOCH {ep}, loss: {train_loss / train_passed:.3f}', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7573cdb5",
   "metadata": {},
   "source": [
    "### Вариант № 3, с nn.rnn (лекционный), для сравнения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fec13a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.1)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00be1a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 0.250, Train loss: 1.957\n",
      "Epoch 1. Time: 0.214, Train loss: 1.702\n",
      "Epoch 2. Time: 0.215, Train loss: 1.634\n",
      "Epoch 3. Time: 0.215, Train loss: 1.594\n",
      "Epoch 4. Time: 0.217, Train loss: 1.565\n",
      "Epoch 5. Time: 0.217, Train loss: 1.542\n",
      "Epoch 6. Time: 0.214, Train loss: 1.522\n",
      "Epoch 7. Time: 0.215, Train loss: 1.504\n",
      "Epoch 8. Time: 0.216, Train loss: 1.488\n",
      "Epoch 9. Time: 0.218, Train loss: 1.473\n"
     ]
    }
   ],
   "source": [
    "for ep in range(10):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "    \n",
    "    X = X.cuda()\n",
    "\n",
    "    for i in range(int(len(X) / 100)):\n",
    "        batch = X[i * 100:(i + 1) * 100]\n",
    "        X_batch = batch[:, :-1]\n",
    "        Y_batch = batch[:, 1:].flatten()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        answers= model(X_batch)\n",
    "        answers = answers.view(-1, len(INDEX_TO_CHAR))\n",
    "        loss = criterion(answers, Y_batch)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "\n",
    "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46beb53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
